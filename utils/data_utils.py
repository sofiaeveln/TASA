import numpy as np
import torch
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import os

def read_meta_datasets(train_cities=[], test_city='', path='',
                       tod_embedding_dim=24, dow_embedding_dim=8, steps_per_day=288):
    """
    ğŸ”§ ä¿®æ­£ï¼šä¿æŒåŸæœ‰input_dim=2çš„æˆåŠŸé…ç½®ï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    data_neural = {c: {} for c in train_cities + [test_city]}
    num_nodes = 0
    
    for c in train_cities + [test_city]:
        dataset = np.load(f'{path}/{c}/dataset.npy')  # (T, N, C)
        adj_mat = np.load(f'{path}/{c}/matrix.npy') if os.path.exists(f'{path}/{c}/matrix.npy') else None

        T, N, C = dataset.shape
        time_idx = np.arange(T)

        # ğŸ”§ å…³é”®ä¿®æ­£ï¼šæ¢å¤ä½¿ç”¨å‰2ä¸ªç‰¹å¾ï¼Œä¿æŒåŸæœ‰ç²¾åº¦
        if C >= 2:
            traffic_data = dataset[:, :, :2]  # ä¿æŒå‰2ä¸ªç‰¹å¾
        else:
            traffic_data = dataset[:, :, :1]  # å¦‚æœåªæœ‰1ä¸ªç‰¹å¾
        
        dataset_processed = traffic_data

        new_features = []
        # TODç‰¹å¾
        if tod_embedding_dim > 0:
            tod_feature = ((time_idx % steps_per_day) / steps_per_day).reshape(T, 1)
            tod_feature = np.tile(tod_feature, (1, N))[:, :, None]
            new_features.append(tod_feature)

        # DOWç‰¹å¾
        if dow_embedding_dim > 0:
            dow_feature = ((time_idx // steps_per_day) % 7).reshape(T, 1)
            dow_feature = np.tile(dow_feature, (1, N))[:, :, None]
            new_features.append(dow_feature)

        if len(new_features) > 0:
            new_features_all = np.concatenate(new_features, axis=-1)
            dataset_processed = np.concatenate([dataset_processed, new_features_all], axis=-1)

        data_neural[c]['dataset'] = dataset_processed
        data_neural[c]['adj_mat'] = adj_mat
        num_nodes = max(num_nodes, dataset_processed.shape[1])
    
    return data_neural, num_nodes

def construct_adj_safe(data, num_node, train_ratio=0.7):
    """
    ğŸ”§ æ–°å¢ï¼šå®‰å…¨çš„é‚»æ¥çŸ©é˜µæ„å»ºï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    # åªä½¿ç”¨è®­ç»ƒé›†æ¯”ä¾‹çš„æ•°æ®æ„å»ºé‚»æ¥çŸ©é˜µ
    total_samples = data.shape[0]
    train_end = int(total_samples * train_ratio)
    train_data = data[:train_end]
    
    # åªä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾ï¼ˆäº¤é€šé€Ÿåº¦ï¼‰
    if len(train_data.shape) == 3:
        train_traffic = train_data[:, :, 0]  # (T, N)
    else:
        train_traffic = train_data  # (T, N)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    train_traffic_normalized = (train_traffic - train_traffic.mean(axis=0, keepdims=True)) / (train_traffic.std(axis=0, keepdims=True) + 1e-8)
    correlation_matrix = np.corrcoef(train_traffic_normalized.T)
    
    # å¤„ç†NaNå€¼
    correlation_matrix = np.nan_to_num(correlation_matrix, nan=0.0)
    
    # è½¬æ¢ä¸ºé‚»æ¥çŸ©é˜µ
    threshold = 0.5
    adj_matrix = (correlation_matrix > threshold).astype(float)
    np.fill_diagonal(adj_matrix, 0)  # å¯¹è§’çº¿è®¾ä¸º0
    
    return adj_matrix

def strict_temporal_split(dataset, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2):
    """
    ğŸ”§ æ–°å¢ï¼šä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²æ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"
    
    total_samples = dataset.shape[0]
    train_end = int(total_samples * train_ratio)
    val_end = int(total_samples * (train_ratio + val_ratio))
    
    return {
        'train': dataset[:train_end],
        'val': dataset[train_end:val_end],
        'test': dataset[val_end:]
    }

def generate_data(dataset, seq_len, horizons, split_ratios=(0.7, 0.1, 0.2), scaler=None):
    """
    ç”Ÿæˆå¤šæ—¶é—´æ­¥çš„é¢„æµ‹æ•°æ®ï¼Œä¿®æ­£æ ‡å‡†åŒ–æµç¨‹é¿å…æ•°æ®æ³„éœ²
    ğŸ”§ ä¿®æ­£ï¼šä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²
    """
    # ğŸ”§ ä¿®æ­£ï¼šå…ˆæŒ‰æ—¶é—´é¡ºåºåˆ†å‰²åŸå§‹æ•°æ®
    temporal_splits = strict_temporal_split(dataset, *split_ratios)
    
    final_splits = {}
    
    for split_name, split_data in temporal_splits.items():
        if len(split_data) == 0:
            final_splits[split_name] = {
                'x': np.array([]), 
                'y': {h: np.array([]) for h in horizons}, 
                'scaler': None
            }
            continue
            
        x, ys = [], {h: [] for h in horizons}
        max_horizon = max(horizons)
        T = len(split_data)
        
        # ç”Ÿæˆåºåˆ—æ•°æ®
        for i in range(T - seq_len - max_horizon):
            a = split_data[i: i + seq_len, :, :]
            skip = False
            for h in horizons:
                # åªä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹å¾ä½œä¸ºé¢„æµ‹ç›®æ ‡
                b = split_data[i + seq_len: i + seq_len + h, :, 0]
                if np.isnan(a).any() or np.isnan(b).any() or np.isinf(a).any() or np.isinf(b).any():
                    skip = True
                    break
                ys[h].append(b)
            if not skip:
                x.append(a)

        if not x:
            final_splits[split_name] = {
                'x': np.array([]), 
                'y': {h: np.array([]) for h in horizons}, 
                'scaler': None
            }
            continue

        x = np.stack(x, axis=0)
        for h in horizons:
            ys[h] = np.stack(ys[h], axis=0)
        
        final_splits[split_name] = {'x': x, 'y': ys, 'scaler': None}
    
    # ğŸ”§ ä¿®æ­£ï¼šåªç”¨è®­ç»ƒé›†æ•°æ®è®­ç»ƒscaler
    if 'train' in final_splits and len(final_splits['train']['x']) > 0:
        new_scaler = StandardScaler()
        train_traffic_data = final_splits['train']['x'][:, :, :, 0:1]  # åªå–ç¬¬ä¸€ä¸ªç‰¹å¾
        train_traffic_2d = train_traffic_data.reshape(-1, 1)
        new_scaler.fit(train_traffic_2d)
        
        # ç”¨è®­ç»ƒé›†ç»Ÿè®¡ä¿¡æ¯æ ‡å‡†åŒ–æ‰€æœ‰åˆ†å‰²çš„æ•°æ®
        for split_name in final_splits:
            if len(final_splits[split_name]['x']) > 0:
                # æ ‡å‡†åŒ–è¾“å…¥æ•°æ®çš„ç¬¬ä¸€ä¸ªç‰¹å¾
                x_data = final_splits[split_name]['x']
                traffic_features = x_data[:, :, :, 0:1]
                traffic_2d = traffic_features.reshape(-1, 1)
                traffic_normalized = new_scaler.transform(traffic_2d)
                traffic_normalized = np.clip(traffic_normalized, -3, 3)
                x_data[:, :, :, 0:1] = traffic_normalized.reshape(traffic_features.shape)
                final_splits[split_name]['x'] = x_data
                
                # æ ‡å‡†åŒ–ç›®æ ‡æ•°æ®
                for h in horizons:
                    if len(final_splits[split_name]['y'][h]) > 0:
                        y_data = final_splits[split_name]['y'][h]
                        y_2d = y_data.reshape(-1, 1)
                        y_norm = new_scaler.transform(y_2d)
                        y_norm = np.clip(y_norm, -3, 3)
                        final_splits[split_name]['y'][h] = y_norm.reshape(y_data.shape)
                
                final_splits[split_name]['scaler'] = new_scaler
    
    return final_splits

def construct_adj_from_coords(coords_path, num_nodes, threshold=0.1):
    """
    ğŸ”§ æ–°å¢ï¼šåŸºäºåœ°ç†åæ ‡æ„å»ºé‚»æ¥çŸ©é˜µï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    if os.path.exists(coords_path):
        coords = np.load(coords_path)  # (N, 2) æˆ– (2, N)
        if coords.shape[0] == 2:
            coords = coords.T  # è½¬æ¢ä¸º (N, 2)
        
        # è®¡ç®—èŠ‚ç‚¹é—´çš„æ¬§å¼è·ç¦»
        distances = np.sqrt(((coords[:, None, :] - coords[None, :, :]) ** 2).sum(axis=2))
        
        # åŸºäºè·ç¦»é˜ˆå€¼æ„å»ºé‚»æ¥çŸ©é˜µ
        adj_matrix = (distances < threshold).astype(float)
        
        # è®¾ç½®å¯¹è§’çº¿ä¸º0ï¼ˆèŠ‚ç‚¹ä¸ä¸è‡ªå·±è¿æ¥ï¼‰
        np.fill_diagonal(adj_matrix, 0)
        
        return adj_matrix
    else:
        # å¦‚æœæ²¡æœ‰åæ ‡æ–‡ä»¶ï¼Œè¿”å›é¢„å®šä¹‰çš„é‚»æ¥çŸ©é˜µï¼ˆä¾‹å¦‚æ ¼çŠ¶å›¾ï¼‰
        adj_matrix = np.eye(num_nodes)
        # æ·»åŠ ç›¸é‚»èŠ‚ç‚¹è¿æ¥ï¼ˆç®€å•çš„é“¾å¼è¿æ¥ï¼‰
        for i in range(num_nodes - 1):
            adj_matrix[i, i + 1] = 1
            adj_matrix[i + 1, i] = 1
        return adj_matrix

def construct_adj(data, num_node):
    """
    ğŸ”§ ä¿®æ­£ï¼šæä¾›å¤‡ç”¨æ–¹æ³•ï¼Œä½†æ¨èä½¿ç”¨åŸºäºåæ ‡çš„æ–¹æ³•
    æ³¨æ„ï¼šæ­¤æ–¹æ³•ä»å¯èƒ½é€ æˆæ•°æ®æ³„éœ²ï¼Œä»…ä½œä¸ºfallback
    """
    print("è­¦å‘Šï¼šä½¿ç”¨åŸºäºæ—¶é—´åºåˆ—çš„é‚»æ¥çŸ©é˜µæ„å»ºå¯èƒ½é€ æˆæ•°æ®æ³„éœ²ï¼Œå»ºè®®ä½¿ç”¨construct_adj_from_coords")
    
    steps_per_day = 288
    days_count = data.shape[0] // steps_per_day
    if days_count == 0:
        data_mean = data.mean(axis=0)
    else:
        data_subset = data[:days_count * steps_per_day]
        data_mean = data_subset.mean(axis=0)
    
    # å°†1Dæ•°ç»„è½¬æ¢ä¸º2Dæ•°ç»„ä»¥é€‚é…cosine_similarity
    data_for_similarity = data_subset.T if days_count > 0 else data.T
    
    tem_matrix = cosine_similarity(data_for_similarity, data_for_similarity)
    tem_matrix = np.exp((tem_matrix - tem_matrix.mean()) / tem_matrix.std())
    return tem_matrix

# å…¶ä»–å‡½æ•°ä¿æŒä¸å˜
def augmentAlign(dist_matrix, auglen):
    """æ ¹æ®ä½™å¼¦ç›¸ä¼¼åº¦å¯¹åˆ†å—è¿›è¡Œå¡«å……"""
    sorted_idx = np.argsort(dist_matrix.reshape(-1) * -1)
    sorted_idx = sorted_idx % dist_matrix.shape[-1]
    augidx = []
    for idx in sorted_idx:
        if idx not in augidx:
            augidx.append(idx)
        if len(augidx) == auglen:
            break
    return np.array(augidx, dtype=int)

def reorderData(parts_idx, mxlen, adj, sps):
    """å¯¹åˆ†å—åçš„æ•°æ®è¿›è¡Œé‡æ’åºå’Œå¡«å……"""
    ori_parts_idx = np.array([], dtype=int)
    reo_parts_idx = np.array([], dtype=int)
    reo_all_idx = np.array([], dtype=int)
    
    for i, part_idx in enumerate(parts_idx):
        part_dist = adj[part_idx, :].copy()
        part_dist[:, part_idx] = 0
        if sps - part_idx.shape[0] > 0:
            local_part_idx = augmentAlign(part_dist, sps - part_idx.shape[0])
            auged_part_idx = np.concatenate([part_idx, local_part_idx], 0)
        else:
            auged_part_idx = part_idx

        reo_parts_idx = np.concatenate([reo_parts_idx, np.arange(part_idx.shape[0]) + sps * i])
        ori_parts_idx = np.concatenate([ori_parts_idx, part_idx])
        reo_all_idx = np.concatenate([reo_all_idx, auged_part_idx])

    return ori_parts_idx, reo_parts_idx, reo_all_idx

def kdTree(locations, times, axis):
    """KDTreeåˆ†å—ç®—æ³•"""
    sorted_idx = np.argsort(locations[axis])
    half = locations.shape[1] // 2
    part1, part2 = np.sort(sorted_idx[:half]), np.sort(sorted_idx[half:])
    parts = []
    
    if times == 1:
        return [part1, part2], max(part1.shape[0], part2.shape[0])
    else:
        left_parts, lmxlen = kdTree(locations[:, part1], times - 1, axis ^ 1)
        right_parts, rmxlen = kdTree(locations[:, part2], times - 1, axis ^ 1)
        for part in left_parts:
            parts.append(part1[part])
        for part in right_parts:
            parts.append(part2[part])
    
    return parts, max(lmxlen, rmxlen)

def compute_node_clusters(num_nodes, num_clusters=None):
    """è®¡ç®—èŠ‚ç‚¹èšç±»"""
    if num_clusters is None:
        num_clusters = min(num_nodes // 10, 100)
    
    nodes_per_cluster = num_nodes // num_clusters
    node_to_cluster = torch.zeros(num_nodes, dtype=torch.long)
    
    for i in range(num_nodes):
        node_to_cluster[i] = min(i // nodes_per_cluster, num_clusters - 1)
    
    return node_to_cluster

class DataLoaderWithScaler:
    """å¸¦æœ‰æ ‡å‡†åŒ–å™¨çš„æ•°æ®åŠ è½½å™¨"""
    def __init__(self, data_batches, scaler=None):
        self.data_batches = data_batches
        self.scaler = scaler
    
    def __iter__(self):
        return iter(self.data_batches)
    
    def __len__(self):
        return len(self.data_batches)

def get_dataloader(splits, batch_size, split='train', shuffle=True, drop_last=False, horizons=[12]):
    """æ ¹æ®æ•°æ®é›†åˆ’åˆ†ç”Ÿæˆæ•°æ®åŠ è½½å™¨ï¼ŒåŒ…å«scalerä¿¡æ¯"""
    split_data = splits[split]
    x, y_tests, scaler = split_data['x'], split_data['y'], split_data.get('scaler', None)
    data_len = x.shape[0]
    num_batches = data_len // batch_size
    if not drop_last:
        num_batches += int((data_len % batch_size) != 0)
    
    data_batches = []
    indices = list(range(data_len))
    if shuffle and split == 'train':
        np.random.shuffle(indices)
    
    for i in range(num_batches):
        start_index = i * batch_size
        end_index = min((i + 1) * batch_size, data_len)
        batch_index = indices[start_index:end_index]
        batch_x = torch.from_numpy(x[batch_index]).float()
        batch_y = {h: torch.from_numpy(y_tests[h][batch_index]).float() for h in horizons}
        data_batches.append((batch_x, batch_y))
    
    data_loader = DataLoaderWithScaler(data_batches, scaler)
    
    return data_loader