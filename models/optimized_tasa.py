import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
import numpy as np
from .semi_self_attention import OptimizedSemiSelfAttentionBlock, LoRAAdapter
from .spatial_transformer import EfficientDualAttBlock

logger = logging.getLogger(__name__)

@dataclass
class OptimizedTASAConfig:
    """ä¼˜åŒ–åçš„TASAé…ç½®"""
    # åŸºæœ¬å‚æ•°
    seed: int = 0
    data: str = ''
    datapath: str = ''
    seq_len: int = 12
    horizons: List[int] = field(default_factory=lambda: [3, 6, 12])
    num_nodes: int = 325
    node_dim: int = 2
    device: torch.device = field(default_factory=lambda: torch.device('cuda:0'))
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    n_layer: int = 4
    n_head: int = 4
    n_embd: int = 64
    dropout: float = 0.1
    bias: bool = True
    n_linear: int = 1
    
    # åµŒå…¥å‚æ•°
    input_dim: int = 1  # ğŸ¯ ä¿®æ”¹ä¸º1ï¼Œåªä½¿ç”¨äº¤é€šæ•°æ®
    tod_embedding_dim: int = 12
    dow_embedding_dim: int = 6
    spatial_embedding_dim: int = 6
    adaptive_embedding_dim: int = 36
    steps_per_day: int = 288
    output_dim: int = 1
    
    # ç©ºé—´åˆ’åˆ†å‚æ•°
    blocksize: int = 8
    blocknum: int = 4
    factors: int = 1
    
    # è®­ç»ƒå‚æ•°
    meta_lr: float = 5e-5
    update_lr: float = 1e-3
    meta_epochs: int = 5
    city_epochs: int = 1
    test_epochs: int = 50
    
    # ä¼˜åŒ–å‚æ•°
    use_lora: bool = True
    lora_rank: int = 16
    use_flash_attn: bool = True
    gradient_checkpointing: bool = True
    
    # é¢†åŸŸç‰¹å®šå‚æ•°
    domain_specific_params: List[str] = field(default_factory=lambda: ['value_mlp', 'value_transform', 'predictors'])
    
    # å±‚æ•°é…ç½®
    temporal_layers: int = 1
    spatial_layers: int = 1
    
    def to_dict(self):
        return {k: v for k, v in self.__dict__.items()}

class OptimizedSTAE(nn.Module):
    """ä¼˜åŒ–çš„ç©ºé—´-æ—¶é—´è‡ªé€‚åº”åµŒå…¥å±‚"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ğŸ”§ å…³é”®ä¿®æ­£ï¼šæ¢å¤åŸå§‹input_dim=2çš„å¤„ç†
        self.input_proj = nn.Linear(config.input_dim, config.n_embd)

        # ğŸ”§ ä¿®æ­£ï¼šæ¢å¤ä¼ ç»ŸåµŒå…¥æ–¹å¼ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
        if config.tod_embedding_dim > 0:
            self.tod_embedding = nn.Embedding(config.steps_per_day, config.tod_embedding_dim)
        else:
            self.tod_embedding = None
            
        if config.dow_embedding_dim > 0:
            self.dow_embedding = nn.Embedding(7, config.dow_embedding_dim)
        else:
            self.dow_embedding = None
        
        # ğŸ”§ ä¿®æ­£ï¼šæ¢å¤åŸå§‹ç©ºé—´åµŒå…¥ï¼Œé¿å…èšç±»ä¸¢å¤±ä¿¡æ¯
        if config.spatial_embedding_dim > 0:
            self.node_emb = nn.Parameter(torch.empty(config.num_nodes, config.spatial_embedding_dim))
            nn.init.xavier_uniform_(self.node_emb)
        else:
            self.node_emb = None
        
        # ğŸ”§ ä¿®æ­£ï¼šæ¢å¤åŸå§‹è‡ªé€‚åº”åµŒå…¥
        if config.adaptive_embedding_dim > 0:
            self.adaptive_embedding = nn.Parameter(
                torch.empty(config.seq_len, config.num_nodes, config.adaptive_embedding_dim)
            )
            nn.init.xavier_uniform_(self.adaptive_embedding)
        else:
            self.adaptive_embedding = None
        
        # ğŸ”§ ä¿®æ­£ï¼šæ¢å¤åŸå§‹ç»´åº¦è®¡ç®—
        self.output_dim = (config.n_embd 
                         + config.tod_embedding_dim 
                         + config.dow_embedding_dim 
                         + config.spatial_embedding_dim 
                         + config.adaptive_embedding_dim)
    
    def _compute_node_clusters(self, num_nodes):
        """è®¡ç®—èŠ‚ç‚¹èšç±»"""
        nodes_per_cluster = max(1, num_nodes // self.num_clusters)
        node_to_cluster = torch.zeros(num_nodes, dtype=torch.long)
        for i in range(num_nodes):
            node_to_cluster[i] = min(i // nodes_per_cluster, self.num_clusters - 1)
        return node_to_cluster
    
    def forward(self, x):
        """ğŸ”§ ä¿®æ­£ï¼šæ¢å¤åŸå§‹åµŒå…¥é€»è¾‘"""
        B, T, N, C = x.size()
        features = []

        # åŸºç¡€inputæŠ•å°„ - æ¢å¤åŸå§‹é€»è¾‘
        main_input = x[..., :self.config.input_dim]  # ç°åœ¨æ˜¯å‰2ä¸ªç‰¹å¾
        main_input = self.input_proj(main_input)
        features.append(main_input)

        # tod embedding - æ¢å¤åŸå§‹é€»è¾‘
        if self.tod_embedding is not None and C > self.config.input_dim:
            tod_idx = (x[..., self.config.input_dim] * self.config.steps_per_day).long().clamp(0, self.config.steps_per_day-1)
            tod_emb = self.tod_embedding(tod_idx)
            features.append(tod_emb)

        # dow embedding - æ¢å¤åŸå§‹é€»è¾‘
        if self.dow_embedding is not None and C > self.config.input_dim+1:
            dow_idx = x[..., self.config.input_dim+1].long().clamp(0, 6)
            dow_emb = self.dow_embedding(dow_idx)
            features.append(dow_emb)

        # spatial embedding - æ¢å¤åŸå§‹é€»è¾‘
        if self.node_emb is not None:
            spatial_emb = self.node_emb.unsqueeze(0).unsqueeze(0).expand(B, T, N, self.config.spatial_embedding_dim)
            features.append(spatial_emb)

        # adaptive embedding - æ¢å¤åŸå§‹é€»è¾‘
        if self.adaptive_embedding is not None:
            adp_emb = self.adaptive_embedding.unsqueeze(0).expand(B, T, N, self.config.adaptive_embedding_dim)
            features.append(adp_emb)

        x = torch.cat(features, dim=-1)
        return x

class CityAdaptationMechanism(nn.Module):
    """åŸå¸‚é€‚åº”æœºåˆ¶"""
    def __init__(self, input_dim: int, hidden_dim: int, lora_rank: int):
        super().__init__()
        # å…±äº«çŸ¥è¯†æå–å™¨
        self.shared_knowledge_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True)
        )
        
        # åŸå¸‚ç‰¹å®šé€‚é…å™¨
        self.city_adapter = LoRAAdapter(hidden_dim, rank=lora_rank)
        
        # èåˆæƒé‡
        self.fusion_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Sigmoid()
        )
    
    def forward(self, x, return_components=False):
        """å‰å‘ä¼ æ’­"""
        shared_knowledge = self.shared_knowledge_extractor(x)
        city_specific = self.city_adapter(shared_knowledge)
        
        concat_features = torch.cat([shared_knowledge, city_specific], dim=-1)
        gate = self.fusion_gate(concat_features)
        output = gate * shared_knowledge + (1 - gate) * city_specific
        
        if return_components:
            return output, {
                'shared': shared_knowledge,
                'specific': city_specific,
                'gate': gate
            }
        return output

class OptimizedTASA(nn.Module):
    def __init__(self, config: OptimizedTASAConfig):
        super().__init__()
        self.config = config

        # 1. åµŒå…¥å±‚
        self.embedding = OptimizedSTAE(config)
        final_dim = self.embedding.output_dim

        # 2. åŸå¸‚é€‚åº”æœºåˆ¶
        self.city_adaptation = CityAdaptationMechanism(
            input_dim=final_dim,
            hidden_dim=config.n_embd,
            lora_rank=config.lora_rank
        )

        # 3. å‚æ•°åˆ†ç¦»æŠ•å½±
        self.shared_proj = nn.Linear(config.n_embd, config.n_embd)
        self.private_proj = nn.Linear(config.n_embd, config.n_embd)

        # 4. æ—¶é—´ Transformer
        self.temporal_blocks = nn.ModuleList([
            OptimizedSemiSelfAttentionBlock(config)
            for _ in range(config.temporal_layers)
        ])

        # 5. ç©ºé—´ Transformer - ä½¿ç”¨æ”¹å›åŸå§‹å®ç°çš„ç‰ˆæœ¬
        self.spatial_transformer = EfficientDualAttBlock(config)

        # 6. è¾“å‡ºå±‚
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.predictors = nn.ModuleDict({
            str(h): nn.Linear(config.n_embd, h * config.output_dim)
            for h in config.horizons
        })

        # 7. åˆå§‹åŒ–
        self._init_weights()

        # 8. é…ç½®å‚æ•°ç»„
        self._configure_parameter_groups()

        logger.info(f"Model initialized with {self.get_num_params()/1e6:.2f}M parameters")

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                if module.weight is not None:
                    torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                if module.weight is not None:
                    torch.nn.init.ones_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                if module.weight is not None:
                    torch.nn.init.normal_(module.weight, mean=0, std=0.02)

    def _configure_parameter_groups(self):
        """é…ç½®å‚æ•°ç»„"""
        self.shared_params = []
        self.private_params = []
        self.domain_specific_params = []
        
        for name, param in self.named_parameters():
            if any(keyword in name for keyword in self.config.domain_specific_params):
                self.domain_specific_params.append(param)
            elif 'shared' in name or 'embedding' in name:
                self.shared_params.append(param)
            else:
                self.private_params.append(param)
        
        logger.info(f"Parameter groups - Shared: {len(self.shared_params)}, "
                   f"Private: {len(self.private_params)}, "
                   f"Domain-specific: {len(self.domain_specific_params)}")
    
    def get_num_params(self):
        """è·å–å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters())
    
    def copy_invariant_params(self, target_model):
        """ä»å…ƒæ¨¡å‹å¤åˆ¶ä¸å˜å‚æ•°åˆ°ç›®æ ‡æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        source_dict = self.state_dict()
        target_dict = target_model.state_dict()
        
        # åªå¤åˆ¶å®Œå…¨åŒ¹é…çš„å‚æ•°
        for name, param in source_dict.items():
            # è·³è¿‡é¢†åŸŸç‰¹å®šå‚æ•°
            if any(keyword in name for keyword in self.config.domain_specific_params):
                continue
                
            # åªå¤åˆ¶å­˜åœ¨ä¸”å½¢çŠ¶å®Œå…¨åŒ¹é…çš„å‚æ•°
            if name in target_dict and param.shape == target_dict[name].shape:
                target_dict[name] = param.clone()
        
        # ä½¿ç”¨strict=Falseå¿½ç•¥ä¸åŒ¹é…çš„å‚æ•°
        target_model.load_state_dict(target_dict, strict=False)
    
    def forward(self, x, y=None, train=True, stage='target',
                ori_parts_idx=None, reo_parts_idx=None, reo_all_idx=None):
        B, T, N, C = x.shape
        
        # 1. åµŒå…¥
        x_embedded = self.embedding(x)
        
        # 2. åŸå¸‚é€‚åº”
        x_adapted = self.city_adaptation(x_embedded)
        
        # 3. å‚æ•°åˆ†ç¦»
        h_shared = self.shared_proj(x_adapted)
        h_private = self.private_proj(x_adapted)
        
        # 4. æ—¶é—´å»ºæ¨¡
        for block in self.temporal_blocks:
            if self.config.gradient_checkpointing and self.training:
                h_private, h_shared = torch.utils.checkpoint.checkpoint(
                    block, h_private, h_shared, 1
                )
            else:
                h_private, h_shared = block(h_private, h_shared, attn_dim=1)
        
        # 5. ç©ºé—´å»ºæ¨¡
        if self.config.gradient_checkpointing and self.training:
            h_private, h_shared = torch.utils.checkpoint.checkpoint(
                self.spatial_transformer, h_private, h_shared
            )
        else:
            h_private, h_shared = self.spatial_transformer(h_private, h_shared)
        
        # 6. èåˆ
        h_final = h_private + h_shared
        h_final = self.ln_f(h_final)
        h_last = h_final[:, -1, :, :]
        
        # 7. é¢„æµ‹
        preds = {}
        for h in self.config.horizons:
            out = self.predictors[str(h)](h_last)
            out = out.view(B, N, h, self.config.output_dim)
            out = out.permute(0, 2, 1, 3).squeeze(-1)
            preds[h] = out
        
        if train and y is not None:
            loss = self.compute_loss(preds, y, stage)
            return preds, loss
        
        return preds
    
    def compute_loss(self, preds, targets, stage='target'):
        """è®¡ç®—æŸå¤±"""
        total_loss = 0
        loss_weights = {'meta': 1.0, 'source': 1.0, 'target': 1.5}
        weight = loss_weights.get(stage, 1.0)
        
        for h in self.config.horizons:
            if h in preds and h in targets:
                l1 = F.l1_loss(preds[h], targets[h])
                l2 = F.mse_loss(preds[h], targets[h])
                total_loss += weight * (l1 + 0.5 * l2)
        
        return total_loss / len(self.config.horizons)
    
    def configure_optimizers(self, weight_decay=0.01, learning_rate=1e-3, 
                             betas=(0.9, 0.999), device_type='cuda'):
        """é…ç½®ä¼˜åŒ–å™¨"""
        decay_params, no_decay_params = [], []
        for name, param in self.named_parameters():
            if param.requires_grad:
                if 'bias' in name or 'ln' in name or 'norm' in name:
                    no_decay_params.append(param)
                else:
                    decay_params.append(param)
        
        optimizer = torch.optim.AdamW(
            [{'params': decay_params, 'weight_decay': weight_decay},
             {'params': no_decay_params, 'weight_decay': 0.0}],
            lr=learning_rate, betas=betas
        )
        return optimizer
    
    @torch.no_grad()
    def evaluate(self, data_loader):
        """è¯„ä¼°æ¨¡å‹ - æ·»åŠ Z-scoreåå½’ä¸€åŒ–å¤„ç†"""
        self.eval()
        preds = {h: [] for h in self.config.horizons}
        targets = {h: [] for h in self.config.horizons}
        
        # ğŸ¯ å…³é”®ä¿®æ”¹ï¼šè·å–scalerç”¨äºåå½’ä¸€åŒ–
        scaler = getattr(data_loader, 'scaler', None)
        
        for x, y in data_loader:
            x = x.to(self.config.device)
            y = {h: y[h].to(self.config.device) for h in self.config.horizons}
            p = self(x, train=False)
            
            for h in self.config.horizons:
                pred = p[h].cpu().numpy()
                target = y[h].cpu().numpy()
                
                # ğŸ¯ å…³é”®ä¿®æ”¹ï¼šè¿›è¡Œåå½’ä¸€åŒ–å¤„ç†
                if scaler is not None:
                    # é¢„æµ‹å€¼åå½’ä¸€åŒ–
                    pred_shape = pred.shape
                    pred_flat = pred.reshape(-1, 1)
                    pred_denorm = scaler.inverse_transform(pred_flat)
                    pred = pred_denorm.reshape(pred_shape)
                    
                    # ç›®æ ‡å€¼åå½’ä¸€åŒ–
                    target_shape = target.shape
                    target_flat = target.reshape(-1, 1)
                    target_denorm = scaler.inverse_transform(target_flat)
                    target = target_denorm.reshape(target_shape)
                
                preds[h].append(pred)
                targets[h].append(target)
        
        results = {}
        for h in self.config.horizons:
            pr = np.concatenate(preds[h], axis=0)
            tr = np.concatenate(targets[h], axis=0)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            mae = np.mean(np.abs(pr - tr))
            mse = np.mean((pr - tr) ** 2)
            rmse = np.sqrt(mse)
            # MAPEè®¡ç®—æ—¶é¿å…é™¤é›¶
            mask = np.abs(tr) > 1e-5
            mape = np.mean(np.abs((tr[mask] - pr[mask]) / tr[mask])) * 100 if mask.any() else 0.0
            
            results[h] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'MAPE': mape}
        
        return results
    
    def visualize_attention(self, x, layer_idx=0):
        """å¯è§†åŒ–æ³¨æ„åŠ›"""
        self.eval()
        with torch.no_grad():
            x_emb = self.embedding(x)
            x_ad, comps = self.city_adaptation(x_emb, return_components=True)
            if layer_idx < len(self.temporal_blocks):
                return comps
            return None